{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS W207 \n",
    "## Final Project - Second Submission\n",
    "## Suhas Gupta, Neha Kumar\n",
    "### 04/06/2019\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Project work completed </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - __ <span style=\"color:blue\"> Data import </span> __\n",
    "    - Import data from competition CSV files into pandas data frame\n",
    "     \n",
    "     \n",
    " - __ <span style=\"color:blue\">Exploratory data analysis</span> __\n",
    "    - Understood data structure, fields, outcome variables\n",
    "    - Developed intuitive understanding of the relationship between outcome variable and independent variables\n",
    "        - Generated scatter plot matrix to understand crime count relationship with day of the week and hour of the day\n",
    "    - Identified categorical and numerical variables\n",
    "\n",
    "\n",
    "- __ <span style=\"color:blue\">Feature Engineering</span> __\n",
    "    - Pruned data set to \n",
    "        - Remove extreme outliers that lead to model fitting problems (e.g. crime categories that have < 0.001% of the total crime count)\n",
    "        - Remove features that contain implausible/incorrect information (e.g. latitude = 90º that indicated North Pole)\n",
    "    - Added new features using sensible correlation and public data:\n",
    "        - Added zip codes using KNN fitting based on latitude & longitude information from the training data set \n",
    "        - Converted composite ‘dates’ feature to separate year, month and hour-of-the-day features\n",
    "    - Encoded categorical and numerical data for using sklearn model development packages\n",
    "        - One-hot encoding for all categorical features\n",
    "        - Scaling and normalisation for numeric features\n",
    "    - Conducted principal component analysis for ranking important features and reducing dimensionality\n",
    "\n",
    "\n",
    "- __ <span style=\"color:blue\">Cross Validation</span> __\n",
    "    - Split single daatset into training and validation sets for cross validation during model training\n",
    "    - Used 80/20 split between training and development data\n",
    "    \n",
    "    \n",
    "- __ <span style=\"color:blue\">Model Training</span> __\n",
    "    - Hyper-parameter search using scikit-learn's GridSearchCV\n",
    "    - Trained and optimised the following classifiers:\n",
    "        - K-Nearest Neighbours (used as a baseline model)\n",
    "        - Multinomial Naive Bayes\n",
    "        - Logistic Regression ( with L1 & L2 normalisation)\n",
    "        - Decision trees\n",
    "        - Random forests\n",
    "\n",
    "\n",
    "- __ <span style=\"color:blue\">Model selection and final prediction</span> __\n",
    "    - Selected random forest classifier for final predictions \n",
    "    - Predicted category on test data \n",
    "    - Generated prediction outcome files in format required for Kaggle submission \n",
    "    - Latest Kaggle score: 25.71 and expected rank: 1976\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Planned future work </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __ <span style=\"color:blue\">EDA</span> __\n",
    "    - Organise all EDA and data import in a specialised class for scalability and abstraction\n",
    "    - Generate more insights in relationship between outcome variable and features\n",
    "\n",
    "\n",
    "- __ <span style=\"color:blue\">Feature Engineering</span> __\n",
    "    - Investigate options to make outcome variable linear \n",
    "        - The outcome variable (crime category count) is heavily skewed (LARCENY and OTHER OFFENSES make up 35% of all counts in training data)\n",
    "        - __Take the Log of the category counts__ to remove this skew (this also helps in satisfying gauss-markov assumptions and make help us in applying logistic regression effectively) \n",
    "    - Add PdDistrict back in the feature set\n",
    "        - It was removed from the earlier models due to a large number of features\n",
    "        - Since we are now using PCA to prune down to important features, we can add all the independent variable back in the training data set \n",
    "    - __Bucket all features into groups based on counts__\n",
    "        - We will create 6 buckets for all the features with approximately 7 features per bucket based on the decreasing order of crime counts. \n",
    "        - This will provide an additional feature to the classifiers and help in separating the very high count features from the very low count ones. \n",
    "        - We expect that this will introduce the non-linearity in the feature set that can help in resolving the undercutting problem that we currently have. _(Classifier will now be able to handle the dominating effect of LARCENY counts that far outweigh the other crimes in the dataset)_\n",
    "         \n",
    "         \n",
    "- __ <span style=\"color:blue\">Model development and scoring</span> __\n",
    "    - Develop function to __compute log-loss metric__ for all classifier predictions (this is the metric used in this Kaggle competition)\n",
    "    - We plan to train all the previous classifiers using the new feature set from #2 above\n",
    "    - __Investigate use of unsupervised learning algorithms__ on this supervised learning problem\n",
    "        - Try Gaussian mixture model on the data to help in clustering of data \n",
    "        - We expect this can help in introducing non linearity in the model to alleviate the problems with heavy skew in the outcome variable\n",
    "    - We plan to investigate a method of __taking votes from multiple classifier predictions__: \n",
    "        - Train top 3 classifiers  based on their performance on the log-loss metric (e.g. MNB, random forest, GMM)\n",
    "        - Predict outcomes on test data using all three trained models\n",
    "        - Final prediction will the majority vote from the above three classifier outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
